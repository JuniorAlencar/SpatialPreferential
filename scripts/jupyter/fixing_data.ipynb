{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.FunctionsProperties import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders to Results\"\"\n",
    "make_results_folders()\n",
    "#move_to_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_properties_file2(N, dim, alpha_a, alpha_g):\n",
    "    # Diretório onde os arquivos estão localizados\n",
    "    path_d = f\"../../data_2/N_{\"\"\n",
    "    # Verificar se o diretório 'prop' existe\n",
    "    if not os.path.exists(path_d):\n",
    "        print(f\"O diretório {path_d} não existe. Nada a ser feito.\")\n",
    "        return\n",
    "    \n",
    "    # Obter todos os arquivos CSV na pasta prop\n",
    "    all_files = glob.glob(os.path.join(path_d, \"*.csv\"))\n",
    "    \n",
    "    # Se não houver arquivos na pasta prop, nada é feito\n",
    "    if not all_files:\n",
    "        print(f\"A pasta {path_d} está vazia. Nada a ser feito.\")\n",
    "        return\n",
    "    \n",
    "    # Checar se o arquivo filenames.txt existe, caso contrário criar um\n",
    "    if os.path.exists(filenames_file):\n",
    "        with open(filenames_file, 'r') as f:\n",
    "            filenames_set = set(f.read().splitlines())  # Ler todos os arquivos já processados\n",
    "    else:\n",
    "        filenames_set = set()\n",
    "    \n",
    "    # Se o arquivo properties_set.txt existir, carregar o dataframe, caso contrário criar um novo\n",
    "    if os.path.exists(properties_file):\n",
    "        df = pd.read_csv(properties_file, sep=' ')\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"#short_path\", \"#diamater\", \"#ass_coeff\"])\n",
    "    \n",
    "    # Variável para rastrear se houve atualizações\n",
    "    updated = False\n",
    "    new_rows = []  # Armazenar novas linhas para adicionar ao dataframe\n",
    "    \n",
    "    # Iterar sobre todos os arquivos CSV e verificar se já foram processados\n",
    "    #block 1\n",
    "    for file in all_files:\n",
    "        filename = os.path.basename(file)\n",
    "        \n",
    "        if os.path.getsize(file) == 0:\n",
    "            print(f\"O arquivo {file} está vazio e será excluído.\")\n",
    "            os.remove(file)\n",
    "            continue  # Pular para o próximo arquivo\n",
    "        \n",
    "        # Se o arquivo já foi processado, ignorar\n",
    "        if filename in filenames_set:\n",
    "            continueentat\n",
    "        new_row = {\n",
    "            \"#short_path\": new_data[\"#mean shortest path\"].values[0],\n",
    "            \"#diamater\": new_data[\"# diamater\"].values[0],\n",
    "            \"#ass_coeff\": new_data[\"#assortativity coefficient\"].values[0]\n",
    "        }\n",
    "        new_rows.append(new_row)\n",
    "        \n",
    "        # Adicionar o nome do arquivo ao conjunto de arquivos processados\n",
    "        filenames_set.add(filename)\n",
    "        updated = True  # Indicar que houve atualizações\n",
    "        #os.remove(file)  # Opcional: remover o arquivo após processamento\n",
    "        \n",
    "    # Se houver atualizações, salvar os arquivos atualizados\n",
    "    if updated:\n",
    "        # Adicionar as novas linhas ao dataframe\n",
    "        df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "        \n",
    "        # Salvar o dataframe atualizado\n",
    "        df.to_csv(properties_file, sep=' ', index=False)\n",
    "        \n",
    "        # Atualizar o arquivo filenames.txt\n",
    "        with open(filenames_file, 'w') as f:\n",
    "            f.write(\"\\n\".join(sorted(filenames_set)))  # Escrever os nomes dos arquivos processados\n",
    "        \n",
    "        print(f\"Arquivos {properties_file} e {filenames_file} atualizados com sucesso.\")\n",
    "    else:\n",
    "        print(\"Nenhuma atualização necessária. Todos os arquivos já estavam processados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def extract_alpha_values2(folder_data):\n",
    "    # Caminho inicial\n",
    "#    base_path = \"../../data_2\"\n",
    "    base_path = folder_data\n",
    "\n",
    "    # Regex para capturar nvalue, dvalue, alpha_a (aavalue) e alpha_g (agvalue)\n",
    "    pattern = r\"N_(\\d+)/dim_(\\d+)/alpha_a_([\\d.]+)_alpha_g_([\\d.]+)\"\n",
    "\n",
    "    # Estrutura para armazenar as combinações encontradas\n",
    "    combinations = set()\n",
    "\n",
    "    # Percorrer todas as subpastas a partir de base_path\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        match = re.search(pattern, root)\n",
    "        if match:\n",
    "            nvalue = int(match.group(1))  # nvalue como inteiro\n",
    "            dvalue = int(match.group(2))  # dvalue como inteiro\n",
    "            aavalue = float(match.group(3))  # alpha_a como float\n",
    "            agvalue = float(match.group(4))  # alpha_g como float\n",
    "            combinations.add((nvalue, dvalue, aavalue, agvalue))\n",
    "    return combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==> READ DOCUMENTATION OF FUNCTION all_properties_file(N,dim, alpha_a, alpha_g) BEFORE RUN THAT CELL!! <==\n",
    "folder_data = \"../../data\"\n",
    "all_combinations_parms =  extract_alpha_values(folder_data)\n",
    "#update_headers(folder_data)\n",
    "for parm in all_combinations_parms:\n",
    "    try:\n",
    "        all_properties_file(parm[0], parm[1], parm[2], parm[3])\n",
    "        #fixing_data(n, d, all_combinations_ag[i][0], all_combinations_ag[i][1])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    #remove_cod_file_column(n, d, all_combinations_ag[i][0], all_combinations_ag[i][1])\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade_headers(file):\n",
    "    # Carregar o CSV em um DataFrame\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "    # Cabeçalhos desejados\n",
    "    cabecalhos_desejados = {\n",
    "        '#short_path': '#mean shortest path',\n",
    "        '#diamater': '# diamater',\n",
    "        '#ass_coeff': '#assortativity coefficient'\n",
    "    }\n",
    "\n",
    "    # Verificar se os cabeçalhos precisam ser atualizados\n",
    "    atualizar = any(col in cabecalhos_desejados and cabecalhos_desejados[col] != col for col in data.columns)\n",
    "\n",
    "    if atualizar:\n",
    "        data.rename(columns=cabecalhos_desejados, inplace=True)\n",
    "        # Salvar o DataFrame atualizado\n",
    "        data.to_csv(file, sep=',', index=False)\n",
    "        print(\"Cabeçalhos atualizados com sucesso!\")\n",
    "    else:\n",
    "        print(\"Os cabeçalhos já estão corretos. Nenhuma alteração foi feita.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.errors import EmptyDataError\n",
    "def all_properties_file(N, dim, alpha_a, alpha_g):\n",
    "    # Diretório onde os arquivos estão localizados\n",
    "    path_d = f\"../../data/N_{N}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/prop\"\n",
    "    path_save = f\"../../data/N_{N}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}\"\n",
    "    print(f\"N = {N}, dim = {dim}, alpha_a = {alpha_a}, alpha_g = {alpha_g}\")\n",
    "    \n",
    "    # Arquivos a serem atualizados\n",
    "    properties_file = os.path.join(path_save, \"properties_set.txt\")\n",
    "    filenames_file = os.path.join(path_save, \"filenames.txt\")\n",
    "    \n",
    "    # Verificar se o diretório 'prop' existe\n",
    "    if not os.path.exists(path_d):\n",
    "        print(f\"O diretório {path_d} não existe. Nada a ser feito.\")\n",
    "        return\n",
    "    \n",
    "    # Obter todos os arquivos CSV na pasta prop\n",
    "    all_files = glob.glob(os.path.join(path_d, \"*.csv\"))\n",
    "    \n",
    "    # Se não houver arquivos na pasta prop, nada é feito\n",
    "    if not all_files:\n",
    "        print(f\"A pasta {path_d} está vazia. Nada a ser feito.\")\n",
    "        return\n",
    "    \n",
    "    # Checar se o arquivo filenames.txt existe, caso contrário criar um\n",
    "    if os.path.exists(filenames_file):\n",
    "        with open(filenames_file, 'r') as f:\n",
    "            filenames_set = set(f.read().splitlines())  # Ler todos os arquivos já processados\n",
    "    else:\n",
    "        filenames_set = set()\n",
    "    \n",
    "    # Se o arquivo properties_set.txt existir, carregar o dataframe, caso contrário criar um novo\n",
    "    if os.path.exists(properties_file):\n",
    "        df = pd.read_csv(properties_file, sep=' ')\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"#short_path\", \"#diamater\", \"#ass_coeff\"])\n",
    "    \n",
    "    # Variável para rastrear se houve atualizações\n",
    "    updated = False\n",
    "    new_rows = []  # Armazenar novas linhas para adicionar ao dataframe\n",
    "    \n",
    "    # Iterar sobre todos os arquivos CSV e verificar se já foram processados\n",
    "    #block 1\n",
    "    for file in all_files:\n",
    "        filename = os.path.basename(file)    \n",
    "        # Se o arquivo já foi processado, ignorar\n",
    "        if filename in filenames_set:\n",
    "            continue\n",
    "        try:\n",
    "            print(file)\n",
    "            # Se o arquivo ainda não foi processado, ler os dados e adicionar ao DataFrame\n",
    "            new_data = pd.read_csv(file)\n",
    "            new_row = {\n",
    "                \"#short_path\": new_data[\"#mean shortest path\"].values[0],\n",
    "                \"#diamater\": new_data[\"# diamater\"].values[0],\n",
    "                \"#ass_coeff\": new_data[\"#assortativity coefficient\"].values[0]\n",
    "            }\n",
    "            new_rows.append(new_row)\n",
    "            \n",
    "            # Adicionar o nome do arquivo ao conjunto de arquivos processados\n",
    "            filenames_set.add(filename)\n",
    "            updated = True  # Indicar que houve atualizações\n",
    "            #os.remove(file)  # Opcional: remover o arquivo após processamento\n",
    "        except EmptyDataError:\n",
    "            print(\"erro\")\n",
    "            os.remove(file)\n",
    "    # Se houver atualizações, salvar os arquivos atualizados\n",
    "    if updated:\n",
    "        # Adicionar as novas linhas ao dataframe\n",
    "        df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "        \n",
    "        # Salvar o dataframe atualizado\n",
    "        df.to_csv(properties_file, sep=' ', index=False)\n",
    "        \n",
    "        # Atualizar o arquivo filenames.txt\n",
    "        with open(filenames_file, 'w') as f:\n",
    "            f.write(\"\\n\".join(sorted(filenames_set)))  # Escrever os nomes dos arquivos processados\n",
    "        \n",
    "        print(f\"Arquivos {properties_file} e {filenames_file} atualizados com sucesso.\")\n",
    "    else:\n",
    "        print(\"Nenhuma atualização necessária. Todos os arquivos já estavam processados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def update_header_in_filenames(base_path):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if \"filenames.txt\" in files:\n",
    "            file_path = os.path.join(root, \"filenames.txt\")\n",
    "            with open(file_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            # Verifica o header atual\n",
    "            if lines and lines[0].strip() == \"filenames\":\n",
    "                print(f\"Trocando header em: {file_path}\")\n",
    "                lines[0] = \"filename\\n\"  # Substitui o header\n",
    "                with open(file_path, \"w\") as f:\n",
    "                    f.writelines(lines)\n",
    "            elif lines and lines[0].strip() == \"filename\":\n",
    "                print(f\"Header já correto em: {file_path}\")\n",
    "            else:\n",
    "                print(f\"Adicionando header em: {file_path}\")\n",
    "                with open(file_path, \"w\") as f:\n",
    "                    f.write(\"filename\\n\")\n",
    "                    f.writelines(lines)\n",
    "\n",
    "# Caminho base da estrutura de pastas\n",
    "base_path = \"../../data\"\n",
    "update_header_in_filenames(base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==> READ DOCUMENTATION OF FUNCTION all_properties_file(N,dim, alpha_a, alpha_g) BEFORE RUN THAT CELL!! <==\n",
    "folder_data = \"../../data\"\n",
    "all_combinations_parms =  extract_alpha_values(folder_data)\n",
    "#update_headers(folder_data)\n",
    "for parm in all_combinations_parms:\n",
    "    # if(parm[0]==160000):\n",
    "    #     N, dim, alpha_a, alpha_g = parm[0], parm[1], parm[2], parm[3]\n",
    "    #     file_path = f\"../../data_2/N_{N}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/prop\"\n",
    "    # N, dim, alpha_a, alpha_g = parm[0], parm[1], parm[2], parm[3]\n",
    "    # file_path = f\"../../data_2/N_{N}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/prop\"\n",
    "    # all_files = glob.glob(os.path.join(file_path,\"*.csv\"))\n",
    "    # for file in all_files:\n",
    "    #     trade_headers(file)\n",
    "    \n",
    "    #all_properties_file(parm[0], parm[1], parm[2], parm[3])\n",
    "    print(f\"{parm[0]}, {parm[1]}, {parm[2]}, {parm[3]}\")\n",
    "    all_properties_dataframe(parm[0], parm[1], parm[2], parm[3])\n",
    "    #all_properties_dataframe(N,dim,alpha_a,alpha_g)\n",
    "        #fixing_data(n, d, all_combinations_ag[i][0], all_combinations_ag[i][1])\n",
    "    \n",
    "    #remove_cod_file_column(n, d, all_combinations_ag[i][0], all_combinations_ag[i][1])\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [5000, 10000, 20000, 40000, 80000, 100000 ,160000, 320000]\n",
    "folder_data = \"../../data\"\n",
    "dim = [1, 2, 3, 4]\n",
    "#format_file(N, dim)\n",
    "all_data(folder_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> Statistical Analysis </b>\n",
    "\n",
    "### For some reason, for some sets of parameters ($N$, $dim$, $\\alpha_a$, $\\alpha_g$) , the values ​​of the assortativity coefficients have a large fluctuation (large STD values), generating averages with a considerably large standard error. With this in mind, it is necessary to perform an analysis and filtering on the data whose sets present these problems.\n",
    "\n",
    "## <b> Using an absolute approach </b>\n",
    "\n",
    "### Set absolute bounds for the standard error and standard deviation ($\\sigma$), regardless of the measurement, and filter out data that exceed these bounds. In my case, i'll use std for other combinations ($\\alpha_g$, $\\alpha_a$) where the standard error are smaller 10%, when $N$ and $dim$ are equals. The process is very simple. Given the set $R$ $=$ $[-0.0245503$, $-0.0155637$, $-0.0219698$, $-0.0144903$ $,...]$ and a good value for $\\sigma$, $\\sigma$ $=$ $\\sigma_{ideal}$. Each value outside of range $-\\sigma_{ideal}$ $<$ $value$ $<$ $\\sigma_{ideal}$ or $value < |\\sigma_{ideal}|$ is refused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>dim</th>\n",
       "      <th>alpha_a</th>\n",
       "      <th>alpha_g</th>\n",
       "      <th>N_samples</th>\n",
       "      <th>short_mean</th>\n",
       "      <th>short_err</th>\n",
       "      <th>short_err_per</th>\n",
       "      <th>diameter_mean</th>\n",
       "      <th>diameter_err</th>\n",
       "      <th>diameter_err_per</th>\n",
       "      <th>ass_coeff_mean</th>\n",
       "      <th>ass_coeff_err</th>\n",
       "      <th>ass_coeff_err_per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12</td>\n",
       "      <td>20.651650</td>\n",
       "      <td>0.561388</td>\n",
       "      <td>2.718368</td>\n",
       "      <td>49.750000</td>\n",
       "      <td>0.905580</td>\n",
       "      <td>1.820262</td>\n",
       "      <td>-0.009978</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>-7.305210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12</td>\n",
       "      <td>20.651650</td>\n",
       "      <td>0.561388</td>\n",
       "      <td>2.718368</td>\n",
       "      <td>49.750000</td>\n",
       "      <td>0.905580</td>\n",
       "      <td>1.820262</td>\n",
       "      <td>-0.009978</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>-7.305210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>160000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12</td>\n",
       "      <td>12.124283</td>\n",
       "      <td>0.154200</td>\n",
       "      <td>1.271827</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>0.509382</td>\n",
       "      <td>1.487246</td>\n",
       "      <td>-0.024083</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>-8.688450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>160000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12</td>\n",
       "      <td>12.124283</td>\n",
       "      <td>0.154200</td>\n",
       "      <td>1.271827</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>0.509382</td>\n",
       "      <td>1.487246</td>\n",
       "      <td>-0.024083</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>-8.688450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12</td>\n",
       "      <td>14.768933</td>\n",
       "      <td>0.169588</td>\n",
       "      <td>1.148276</td>\n",
       "      <td>38.833333</td>\n",
       "      <td>0.694495</td>\n",
       "      <td>1.788399</td>\n",
       "      <td>-0.040293</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>-5.257305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        N  dim  alpha_a  alpha_g  N_samples  short_mean  short_err  \\\n",
       "0  160000    1      2.0     14.0         12   20.651650   0.561388   \n",
       "1  160000    1      2.0     14.0         12   20.651650   0.561388   \n",
       "2  160000    1      0.0      2.0         12   12.124283   0.154200   \n",
       "3  160000    1      0.0      2.0         12   12.124283   0.154200   \n",
       "4  160000    1      1.0      2.0         12   14.768933   0.169588   \n",
       "\n",
       "   short_err_per  diameter_mean  diameter_err  diameter_err_per  \\\n",
       "0       2.718368      49.750000      0.905580          1.820262   \n",
       "1       2.718368      49.750000      0.905580          1.820262   \n",
       "2       1.271827      34.250000      0.509382          1.487246   \n",
       "3       1.271827      34.250000      0.509382          1.487246   \n",
       "4       1.148276      38.833333      0.694495          1.788399   \n",
       "\n",
       "   ass_coeff_mean  ass_coeff_err  ass_coeff_err_per  \n",
       "0       -0.009978       0.000729          -7.305210  \n",
       "1       -0.009978       0.000729          -7.305210  \n",
       "2       -0.024083       0.002092          -8.688450  \n",
       "3       -0.024083       0.002092          -8.688450  \n",
       "4       -0.040293       0.002118          -5.257305  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataframe with all data\n",
    "df = pd.read_csv(\"../../data/all_data.txt\", delimiter=\" \")\n",
    "# Filter all data for alpha_a > 1.0\n",
    "df = df[df[\"alpha_g\"]>=1.0]\n",
    "# Save new dataframe\n",
    "df.to_csv(\"../../data/all_data.txt\", index=False, sep=' ')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1821715368.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Linha {row.Index}: {row.#short_path}, {row.#diamater}, {row.#ass_coeff}\")\u001b[0m\n\u001b[0m                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "N = 160000\n",
    "df_N = df[(df[\"N\"]==N) & (df[\"alpha_g\"]==2.0)]\n",
    "alpha_a = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5]\n",
    "for row in df_N.itertuples(index=True):\n",
    "    print(f\"Linha {row.Index}: {row.#short_path}, {row.#diamater}, {row.#ass_coeff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter N to future linear regression Propetie = Xi_Propertie(dim, alpha_a, alpha_g) * log_10(N) + Chi_Propertie(dim, alpha_a, alpha_g)\n",
    "N = [5000, 10000, 20000, 40000, 80000, 160000, 320000]\n",
    "alpha_filter = [0.0, 1.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0]\n",
    "dimensions = [1, 2, 3, 4]\n",
    "\n",
    "# Filtered dataframe with these sets\n",
    "f_df = df[\n",
    "    (df['N'].isin(N)) &\n",
    "    (df['alpha_a'].isin(alpha_filter)) &\n",
    "    (df['dim'].isin(dimensions)) &\n",
    "    (df['alpha_g'] == 2.0)\n",
    "].sort_values(by=[\"alpha_a\"]).reset_index(drop=True)\n",
    "f_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with empty dictionary\n",
    "data_percent = {\"N\": [], \"dim\": [], \"alpha_a\": [], \"alpha_g\": [], \"N_samples\":[] ,\"per_short\": \n",
    "                [],\"std_short\":[] ,\"per_ass\": [],\"std_ass\":[] ,\"per_dia\": [], \"std_dia\":[]}\n",
    "\n",
    "# Iterate through the rows of the DataFrame\n",
    "for _, row in f_df.iterrows():\n",
    "    data_percent[\"N\"].append(row[\"N\"])\n",
    "    data_percent[\"dim\"].append(row[\"dim\"])\n",
    "    data_percent[\"alpha_a\"].append(row[\"alpha_a\"])\n",
    "    data_percent[\"alpha_g\"].append(row[\"alpha_g\"])\n",
    "    data_percent[\"N_samples\"].append(row[\"N_samples\"])\n",
    "    \n",
    "    data_percent[\"std_short\"].append(row[\"short_std\"])\n",
    "    data_percent[\"std_ass\"].append(row[\"ass_coeff_std\"])\n",
    "    data_percent[\"std_dia\"].append(row[\"diameter_std\"])\n",
    "    \n",
    "    # calculating percentages of each standard error\n",
    "    per_short = (row[\"short_err\"] / row[\"short_mean\"]) * 100 if row[\"short_mean\"] != 0 else 0\n",
    "    per_ass = (row[\"ass_coeff_err\"] / abs(row[\"ass_coeff_mean\"])) * 100 if row[\"ass_coeff_mean\"] != 0 else 0\n",
    "    per_dia = (row[\"diameter_err\"] / row[\"diameter_mean\"]) * 100 if row[\"diameter_mean\"] != 0 else 0\n",
    "    \n",
    "    data_percent[\"per_short\"].append(per_short)\n",
    "    data_percent[\"per_ass\"].append(per_ass)\n",
    "    data_percent[\"per_dia\"].append(per_dia)\n",
    "\n",
    "# New dataframe with all percentages to each properties\n",
    "df_per = pd.DataFrame(data=data_percent)\n",
    "df_per.to_csv(\"percent_err.txt\", index=False, sep=' ')\n",
    "df_per.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering just percentages with values greater than 10\n",
    "filtered_df = df_per[\n",
    "    (df_per['per_short'] >= 10) | \n",
    "    (df_per['per_ass'] >= 10) | \n",
    "    (df_per['per_dia'] >= 10)\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just assortativity with % standard error >= 10%\n",
    "filtered_df[filtered_df[\"per_short\"]>=10],filtered_df[filtered_df[\"per_dia\"]>=10], filtered_df[filtered_df[\"per_ass\"]>=10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the general dataframe (df), with parameters other than those whose error is greater than 10%\n",
    "N_filt = [i for i in filtered_df[\"N\"]]\n",
    "dim_filt = [i for i in filtered_df[\"dim\"]]\n",
    "alpha_a_filt = [i for i in filtered_df[\"alpha_a\"]]\n",
    "alpha_g_filt = [i for i in filtered_df[\"alpha_g\"]]\n",
    "N_samples_filt = [i for i in filtered_df[\"N_samples\"]]\n",
    "\n",
    "comb = {\"N\":N_filt, \"dim\": dim_filt, \"alpha_a\":alpha_a_filt, \"alpha_g\":alpha_g_filt, \"N_samples\":N_samples_filt}\n",
    "df_remover = pd.DataFrame(comb)\n",
    "\n",
    "df_non_10 = df.merge(df_remover, on=['N', 'dim', 'alpha_a', 'alpha_g', \"N_samples\"], how='left', indicator=True)\n",
    "df_non_10 = df_non_10[df_non_10['_merge'] == 'left_only'].drop(columns=['_merge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_10[df_non_10[\"N\"]==40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with % error in assortativity greater 10%\n",
    "filtered_df[filtered_df[\"N\"]==5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parameters = {\"N\": [], \"dim\": [], \"alpha_a\": [], \"alpha_g\": [], \"N_samples\":[]}\n",
    "\n",
    "for _, row in filtered_df.iterrows():\n",
    "    data_parameters[\"N\"].append(int(row[\"N\"]))\n",
    "    data_parameters[\"dim\"].append(int(row[\"dim\"]))\n",
    "    data_parameters[\"alpha_a\"].append(float(row[\"alpha_a\"]))\n",
    "    data_parameters[\"alpha_g\"].append(float(row[\"alpha_g\"]))\n",
    "    if(row[\"N\"] == 5000):\n",
    "        data_parameters[\"N_samples\"].append(20000)\n",
    "    elif(row[\"N\"] == 10000):\n",
    "        data_parameters[\"N_samples\"].append(3200)\n",
    "    elif(row[\"N\"] == 20000):\n",
    "        data_parameters[\"N_samples\"].append(700)\n",
    "    elif(row[\"N\"] == 40000):\n",
    "        data_parameters[\"N_samples\"].append(500)\n",
    "    elif(row[\"N\"] == 80000):\n",
    "        data_parameters[\"N_samples\"].append(200)\n",
    "    elif(row[\"N\"] == 160000):\n",
    "        data_parameters[\"N_samples\"].append(30)\n",
    "    elif(row[\"N\"] == 320000):\n",
    "        data_parameters[\"N_samples\"].append(15)\n",
    "\n",
    "df_run_multi = pd.DataFrame(data=data_parameters)\n",
    "df_run_multi.to_csv(\"../python/run_multi.txt\", index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = []\n",
    "combinations = extract_alpha_values2(\"../../data_2\")\n",
    "for parms in combinations:\n",
    "    N, dim, alpha_a, alpha_g = parms[0], parms[1], parms[2], parms[3]\n",
    "    files = f\"../../data_2/N_{N}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/properties_set.txt\"\n",
    "    if(os.path.exists(files)):\n",
    "        df = pd.read_csv(files,delimiter=' ')\n",
    "        if \"#ass_coeff\" in df.columns:\n",
    "            R = df[\"#ass_coeff\"]\n",
    "            r_per = abs(R.sem() / R.mean()) * 100\n",
    "            #all_properties_file2(N, dim, alpha_a, alpha_g)\n",
    "            \n",
    "            #if(N==320000):\n",
    "                #print(f\"dim = {dim}, alpha_a = {alpha_a}, alpha_g = {alpha_g}, N = {N}, N_samples = {len(R)}, r_per = {r_per}\")\n",
    "            if (30 > r_per > 10):\n",
    "                if (N==320000):\n",
    "                    #all_properties_file2(N, dim, alpha_a, alpha_g)\n",
    "                    #remove_outliers(N, dim, alpha_a, alpha_g)\n",
    "                    print(f\"dim = {dim}, alpha_a = {alpha_a}, alpha_g = {alpha_g}, N = {N}, N_samples = {len(R)}, r_per = {r_per}\")\n",
    "    # else:\n",
    "    #     all_files = glob.glob(os.path.join(f\"../../data_2/N_{N}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/\", \"*.csv\"))\n",
    "    #     #for file in all_files:\n",
    "    #         #trade_headers(file)\n",
    "    #     all_properties_file2(N, dim, alpha_a, alpha_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with % error in assortativity less than 10%\n",
    "df_non_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [5000, 10000, 20000, 40000, 80000, 160000, 320000]\n",
    "# get σ_ideal\n",
    "mean_std_N_for_dim = {\"N\":[], \"dim\":[], \"ass_coeff_std_mean\":[]}\n",
    "for n in N:\n",
    "    for dim in dimensions:\n",
    "        df_N_dim = df_non_10[(df_non_10[\"N\"]==n) & (df_non_10[\"dim\"]==dim) & (df_non_10[\"alpha_g\"]==2.0)]\n",
    "        \n",
    "        mean_std_N_for_dim[\"N\"].append(n)\n",
    "        mean_std_N_for_dim[\"dim\"].append(dim)\n",
    "        mean_std_N_for_dim[\"ass_coeff_std_mean\"].append(df_N_dim[\"ass_coeff_std\"].mean())\n",
    "\n",
    "# list of mean σ to set with σ_ideal smallest 10% for each combinations (N, dim)\n",
    "df_std_N_dim = pd.DataFrame(data=mean_std_N_for_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set with σ_ideal for each (N, dim)\n",
    "df_std_N_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = filtered_df[filtered_df[\"N\"]==5000]\n",
    "df_10 = filtered_df[filtered_df[\"N\"]==10000]\n",
    "df_20 = filtered_df[filtered_df[\"N\"]==20000]\n",
    "df_40 = filtered_df[filtered_df[\"N\"]==40000]\n",
    "df_80 = filtered_df[filtered_df[\"N\"]==80000]\n",
    "df_160 = filtered_df[filtered_df[\"N\"]==160000]\n",
    "df_320 = filtered_df[filtered_df[\"N\"]==320000]\n",
    "#N = 10000,  n_s 3200\n",
    "#N = 20000,  n_s 700\n",
    "#N = 40000,  n_s 500\n",
    "#N = 80000,  n_s 200\n",
    "#N = 160000, n_s 30\n",
    "#N = 320000, n_s 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [5000, 10000, 20000, 40000, 160000]\n",
    "count = 0\n",
    "for n in N:\n",
    "    df_f = filtered_df[filtered_df[\"N\"]==n]\n",
    "    for _, row in df_f.iterrows():\n",
    "        dim = int(row[\"dim\"])\n",
    "        alpha_a, alpha_g = row[\"alpha_a\"], row[\"alpha_g\"]\n",
    "        path_file = f\"../../data_2/N_{n}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/properties_set.txt\"\n",
    "        source_path = f\"../../data_2/N_{n}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/\"\n",
    "        df = pd.read_csv(path_file, sep=' ')\n",
    "        R = df[\"#ass_coeff\"]\n",
    "        R_per = (R.sem()/abs(R.mean()))*100\n",
    "        if(R_per > 10):\n",
    "            count +=1\n",
    "            print(f\"alpha_a = {alpha_a}, alpha_g = {alpha_g}, dim = {dim}, err = {R_per}, N = {n}, N_s = {len(R)}\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_alpha_values2(folder_data):\n",
    "    # Caminho inicial\n",
    "#    base_path = \"../../data_2\"\n",
    "    base_path = folder_data\n",
    "\n",
    "    # Regex para capturar nvalue, dvalue, alpha_a (aavalue) e alpha_g (agvalue)\n",
    "    pattern = r\"N_(\\d+)/dim_(\\d+)/alpha_a_([\\d.]+)_alpha_g_([\\d.]+)\"\n",
    "\n",
    "    # Estrutura para armazenar as combinações encontradas\n",
    "    combinations = set()\n",
    "\n",
    "    # Percorrer todas as subpastas a partir de base_path\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        match = re.search(pattern, root)\n",
    "        if match:\n",
    "            nvalue = int(match.group(1))  # nvalue como inteiro\n",
    "            dvalue = int(match.group(2))  # dvalue como inteiro\n",
    "            aavalue = float(match.group(3))  # alpha_a como float\n",
    "            agvalue = float(match.group(4))  # alpha_g como float\n",
    "            combinations.add((nvalue, dvalue, aavalue, agvalue))\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parms = extract_alpha_values2(\"../../data_2\")\n",
    "for parms in all_parms:\n",
    "    path_prop = f\"../../data_3/N_{parms[0]}/dim_{parms[1]}/alpha_a_{parms[2]}_alpha_g_{parms[3]}/prop\"\n",
    "    # Verificar se a pasta existe\n",
    "    if os.path.exists(path_prop):\n",
    "        # Listar todos os arquivos na pasta\n",
    "        for file_name in os.listdir(path_prop):\n",
    "            file_path = os.path.join(path_prop, file_name)\n",
    "            # Verificar se é um arquivo antes de excluir\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "                print(f\"Arquivo '{file_path}' removido com sucesso.\")\n",
    "        print(f\"Todos os arquivos em '{path_prop}' foram removidos.\")\n",
    "    else:\n",
    "        print(f\"A pasta '{path_prop}' não existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40000\n",
    "dim, alpha_a, alpha_g = 4, 10.0, 2.0\n",
    "all_properties_file2(n, dim, alpha_a, alpha_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path_file = f\"../../data_2/N_{n}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/properties_set.txt\"\n",
    "# Criar um DataFrame\n",
    "df = pd.read_csv(path_file, sep=' ')\n",
    "R = df[\"#ass_coeff\"]\n",
    "r_mean, r_std, r_err = R.mean(), R.std(), R.sem()\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "ax.errorbar(r_mean,1,xerr=r_std,marker='o',lw=1.4, capsize=10, capthick=1.4)\n",
    "R_in = [i for i in R if abs(i-r_mean) <= r_std]\n",
    "R_out = [i for i in R if abs(i-r_mean) > r_std]\n",
    "\n",
    "ax.plot(R_in,np.ones(len(R_in)),'o', color='blue')\n",
    "ax.plot(R_out,np.ones(len(R_out)),'o', color='green')\n",
    "#ax.plot(R,np.ones(len(R)),'o')\n",
    "#ax.set_xlim([r_mean - r_err, r_mean + r_err])\n",
    "\n",
    "std_in = np.std(np.array(R_in), ddof=1)\n",
    "\n",
    "R_in_err = std_in / np.sqrt(len(R_in))\n",
    "\n",
    "print(\"R_size:\",len(R))\n",
    "print(\"R_in_size:\",len(R_in))\n",
    "print(\"R_out_size:\",len(R_out))\n",
    "print(\"R_ratio:\",len(R_in)/len(R_out))\n",
    "print(\"R_err:\", (r_err/abs(r_mean))*100)\n",
    "print(\"R_mean_before:\", r_mean)\n",
    "print(\"R_mean_after:\", np.mean(np.array(R_in)))\n",
    "print(\"R_err_after:\", (R_in_err) / abs(np.mean(R_in))*100 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, dim, alpha_a, alpha_g = 5000, 1, 3.0, 2.0\n",
    "file_path = f\"../../data_2/N_{n}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/properties_set.txt\"\n",
    "# Criar um DataFrame\n",
    "#df = pd.read_csv(path_file, sep=' ')\n",
    "df = pd.read_csv(file_path, delim_whitespace=True)\n",
    "R = df[\"#ass_coeff\"]\n",
    "print(df)\n",
    "num_samples = float(len(R))\n",
    "num_points = 20\n",
    "divisions = int(num_samples // num_points)\n",
    "width = [(0,i*divisions) for i in range(1,num_points)]\n",
    "\n",
    "R_err = []\n",
    "R_mean = []\n",
    "R_std = []\n",
    "n_samples = []\n",
    "C_V = []\n",
    "\n",
    "for i in range(len(width)):\n",
    "    R_err.append(R[0:width[i][1]].sem())\n",
    "    R_mean.append(R[0:width[i][1]].mean())\n",
    "    \n",
    "    r_std = abs(R[0:width[i][1]].std())\n",
    "    r_mean = abs(R[0:width[i][1]].mean())\n",
    "    R_std.append(r_std)\n",
    "    #print(r_std, r_mean)\n",
    "    #C_V.append((/)*100)\n",
    "    n_samples.append(len(R[0:width[i][1]]))\n",
    "\n",
    "#print(R_std)\n",
    "#print(R_mean)\n",
    "#print(R_err)\n",
    "print(\"N_samples:\", len(R))\n",
    "print(f\"R_err = {abs(R.sem()/R.mean())*100} %\" )\n",
    "percent = [abs(i/j)*100 for i,j in zip(R_err, R_mean)]\n",
    "#print([i/j for i,j in zip(R_err, R_mean)])\n",
    "#plt.plot(n_samples, R_err,'o', label='errors')\n",
    "plt.plot(n_samples, percent,'o', label=f'errors, err_minimum = {min(percent)} %')\n",
    "plt.axhline(y=10, color='red', linestyle='--', linewidth=2, label=\"y=10%\")  # Linha horizontal\n",
    "#plt.plot(n_samples, R_mean,'o', label='means')\n",
    "#plt.plot(n_samples, C_V,'o', label='means')\n",
    "#plt.axhline(y=np.mean(np.array(C_V)), xmin=min(n_samples),xmax=max(n_samples))\n",
    "plt.ylabel(\"R_error(%)\")\n",
    "plt.xlabel(\"N_samples\")\n",
    "plt.ylim([0, 100])\n",
    "plt.xlim([0, width[-1][1]])\n",
    "plt.legend()\n",
    "#plt.xscale(\"log\")\n",
    "#plt.savefig(\"CV_n_samples.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, dim, alpha_a, alpha_g = 5000, 3, 12.0, 2.0\n",
    "all_properties_file2(N, dim, alpha_a, alpha_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, dim, alpha_a, alpha_g = 40000, 4, 10.0, 2.0\n",
    "file_path = f\"../../data_2/N_{n}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/properties_set.txt\"\n",
    "# Criar um DataFrame\n",
    "#df = pd.read_csv(path_file, sep=' ')\n",
    "df = pd.read_csv(file_path, delim_whitespace=True)\n",
    "R = df[\"#ass_coeff\"]\n",
    "\n",
    "num_samples = float(len(R))\n",
    "num_points = 20\n",
    "divisions = int(num_samples // num_points)\n",
    "width = [(0,i*divisions) for i in range(1,num_points)]\n",
    "\n",
    "R_err = []\n",
    "R_mean = []\n",
    "R_std = []\n",
    "n_samples = []\n",
    "C_V = []\n",
    "\n",
    "for i in range(len(width)):\n",
    "    R_err.append(R[0:width[i][1]].sem())\n",
    "    R_mean.append(R[0:width[i][1]].mean())\n",
    "    \n",
    "    r_std = abs(R[0:width[i][1]].std())\n",
    "    r_mean = abs(R[0:width[i][1]].mean())\n",
    "    R_std.append(r_std)\n",
    "    #print(r_std, r_mean)\n",
    "    #C_V.append((/)*100)\n",
    "    n_samples.append(len(R[0:width[i][1]]))\n",
    "\n",
    "#print(R_std)\n",
    "#print(R_mean)\n",
    "#print(R_err)\n",
    "print(\"N_samples:\", len(R))\n",
    "print(f\"R_err = {abs(R.sem()/R.mean())*100} %\" )\n",
    "percent = [abs(i/j)*100 for i,j in zip(R_err, R_mean)]\n",
    "#print([i/j for i,j in zip(R_err, R_mean)])\n",
    "#plt.plot(n_samples, R_err,'o', label='errors')\n",
    "plt.plot(n_samples, percent,'o', label=f'errors, err_minimum = {min(percent)} %')\n",
    "plt.axhline(y=10, color='red', linestyle='--', linewidth=2, label=\"y=10%\")  # Linha horizontal\n",
    "#plt.plot(n_samples, R_mean,'o', label='means')\n",
    "#plt.plot(n_samples, C_V,'o', label='means')\n",
    "#plt.axhline(y=np.mean(np.array(C_V)), xmin=min(n_samples),xmax=max(n_samples))\n",
    "plt.ylabel(\"R_error(%)\")\n",
    "plt.xlabel(\"N_samples\")\n",
    "plt.ylim([0, 100])\n",
    "plt.xlim([0, width[-1][1]])\n",
    "plt.legend()\n",
    "#plt.xscale(\"log\")\n",
    "#plt.savefig(\"CV_n_samples.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[filtered_df[\"N\"]==10000].reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, dim, alpha_a, alpha_g = 5000, 3, 12.0, 2.0\n",
    "file_path = f\"../../data_2/N_{n}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/properties_set.txt\"\n",
    "# Criar um DataFrame\n",
    "#df = pd.read_csv(path_file, sep=' ')\n",
    "df = pd.read_csv(file_path, sep=' ')\n",
    "R = df[\"#ass_coeff\"]\n",
    "print(df)\n",
    "num_samples = float(len(R))\n",
    "num_points = 20\n",
    "divisions = int(num_samples // num_points)\n",
    "width = [(0,i*divisions) for i in range(1,num_points)]\n",
    "\n",
    "R_err = []\n",
    "R_mean = []\n",
    "R_std = []\n",
    "n_samples = []\n",
    "C_V = []\n",
    "\n",
    "for i in range(len(width)):\n",
    "    R_err.append(R[0:width[i][1]].sem())\n",
    "    R_mean.append(R[0:width[i][1]].mean())\n",
    "    \n",
    "    r_std = abs(R[0:width[i][1]].std())\n",
    "    r_mean = abs(R[0:width[i][1]].mean())\n",
    "    R_std.append(r_std)\n",
    "    #print(r_std, r_mean)\n",
    "    #C_V.append((/)*100)\n",
    "    n_samples.append(len(R[0:width[i][1]]))\n",
    "\n",
    "#print(R_std)\n",
    "#print(R_mean)\n",
    "#print(R_err)\n",
    "print(\"N_samples:\", len(R))\n",
    "print(f\"R_err = {abs(R.sem()/R.mean())*100} %\" )\n",
    "percent = [abs(i/j)*100 for i,j in zip(R_err, R_mean)]\n",
    "#print([i/j for i,j in zip(R_err, R_mean)])\n",
    "#plt.plot(n_samples, R_err,'o', label='errors')\n",
    "plt.plot(n_samples, percent,'o', label=f'errors, err_minimum = {min(percent)} %')\n",
    "plt.axhline(y=10, color='red', linestyle='--', linewidth=2, label=\"y=10%\")  # Linha horizontal\n",
    "#plt.plot(n_samples, R_mean,'o', label='means')\n",
    "#plt.plot(n_samples, C_V,'o', label='means')\n",
    "#plt.axhline(y=np.mean(np.array(C_V)), xmin=min(n_samples),xmax=max(n_samples))\n",
    "plt.ylabel(\"R_error(%)\")\n",
    "plt.xlabel(\"N_samples\")\n",
    "#plt.ylim([0, 100])\n",
    "plt.xlim([0, width[-1][1]])\n",
    "plt.legend()\n",
    "#plt.xscale(\"log\")\n",
    "#plt.savefig(\"CV_n_samples.pdf\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "dim, alpha_a, alpha_g = 1, 8.0, 3.0\n",
    "path_file = f\"../../data/N_{n}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/properties_set.txt\"\n",
    "# Criar um DataFrame\n",
    "df = pd.read_csv(path_file, sep=',')\n",
    "R = df[\"#ass_coeff\"]\n",
    "\n",
    "r_mean, r_std, r_err = R.mean(), R.std(), R.sem()\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "ax.errorbar(r_mean,1,xerr=r_std,marker='o',lw=1.4, capsize=14 ,capthick=1.4)\n",
    "\n",
    "R_in = [i for i in R if abs(i-r_mean) <= r_std]\n",
    "R_out = [i for i in R if abs(i-r_mean) > r_std]\n",
    "\n",
    "ax.plot(R_in,np.ones(len(R_in)),'o', color='blue')\n",
    "ax.plot(R_out,np.ones(len(R_out)),'o', color='green')\n",
    "\n",
    "std_in = np.std(np.array(R_in), ddof=1)\n",
    "\n",
    "R_in_err = std_in / np.sqrt(len(R_in))\n",
    "\n",
    "print(\"R_size:\",len(R))\n",
    "print(\"σ:\", r_std)\n",
    "print(\"R_mean:\", r_mean)\n",
    "print(\"R_in_size:\",len(R_in))\n",
    "print(\"R_out_size:\",len(R_out))\n",
    "print(\"R_ratio:\",len(R_in)/len(R_out))\n",
    "print(\"R_err:\", (r_err/abs(r_mean))*100)\n",
    "print(\"R_mean_in:\", np.mean(np.array(R_in)))\n",
    "\n",
    "print(\"R_err_int:\", (R_in_err) / abs(np.mean(R_in))*100 )\n",
    "#ax.set_xlim([r_mean - r_err, r_mean + r_err])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box_plot\n",
    "n=320000\n",
    "dim = 1\n",
    "alpha_a, alpha_g = 0.0, 2.0\n",
    "path_file = f\"../../data/N_{n}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/properties_set.txt\"\n",
    "a = pd.read_csv(path_file, sep=',')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(a[\"#ass_coeff\"], vert=True, patch_artist=True, showmeans=True)\n",
    "plt.title(\"Box Plot for #ass_coeff\", fontsize=14)\n",
    "plt.ylabel(\"#ass_coeff\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[filtered_df[\"N\"]==320000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20000\n",
    "dim = [2, 3, 4, 3, 4, 4]\n",
    "alpha_a = [6.0, 9.0, 10.0, 10.0, 11.0, 12.0]\n",
    "alpha_g = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, dim, alpha_a, alpha_g = 40000, 3, 8.0, 2.0\n",
    "file_path = f\"../../data_2/N_{n}/dim_{dim}/alpha_a_{alpha_a}_alpha_g_{alpha_g}/properties_set.txt\"\n",
    "# Criar um DataFrame\n",
    "#df = pd.read_csv(path_file, sep=' ')\n",
    "df = pd.read_csv(file_path, sep=' ')\n",
    "R = df[\"#ass_coeff\"]\n",
    "print(df)\n",
    "num_samples = float(len(R))\n",
    "num_points = 20\n",
    "divisions = int(num_samples // num_points)\n",
    "width = [(0,i*divisions) for i in range(1,num_points)]\n",
    "\n",
    "R_err = []\n",
    "R_mean = []\n",
    "R_std = []\n",
    "n_samples = []\n",
    "C_V = []\n",
    "\n",
    "for i in range(len(width)):\n",
    "    R_err.append(R[0:width[i][1]].sem())\n",
    "    R_mean.append(R[0:width[i][1]].mean())\n",
    "    \n",
    "    r_std = abs(R[0:width[i][1]].std())\n",
    "    r_mean = abs(R[0:width[i][1]].mean())\n",
    "    R_std.append(r_std)\n",
    "    #print(r_std, r_mean)\n",
    "    #C_V.append((/)*100)\n",
    "    n_samples.append(len(R[0:width[i][1]]))\n",
    "\n",
    "#print(R_std)\n",
    "#print(R_mean)\n",
    "#print(R_err)\n",
    "print(\"N_samples:\", len(R))\n",
    "print(f\"R_err = {abs(R.sem()/R.mean())*100} %\" )\n",
    "percent = [abs(i/j)*100 for i,j in zip(R_err, R_mean)]\n",
    "#print([i/j for i,j in zip(R_err, R_mean)])\n",
    "#plt.plot(n_samples, R_err,'o', label='errors')\n",
    "plt.plot(n_samples, percent,'o', label=f'errors, err_minimum = {min(percent)} %')\n",
    "plt.axhline(y=10, color='red', linestyle='--', linewidth=2, label=\"y=10%\")  # Linha horizontal\n",
    "#plt.plot(n_samples, R_mean,'o', label='means')\n",
    "#plt.plot(n_samples, C_V,'o', label='means')\n",
    "#plt.axhline(y=np.mean(np.array(C_V)), xmin=min(n_samples),xmax=max(n_samples))\n",
    "plt.ylabel(\"R_error(%)\")\n",
    "plt.xlabel(\"N_samples\")\n",
    "#plt.ylim([0, 100])\n",
    "plt.xlim([0, width[-1][1]])\n",
    "plt.legend()\n",
    "#plt.xscale(\"log\")\n",
    "#plt.savefig(\"CV_n_samples.pdf\"')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
